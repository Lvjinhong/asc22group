{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example running requirements: you need to install cirq with version 0.7.0 to load the Sycamore circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cirq'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mload_circuits\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QuantumCircuit\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01martensor\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      3\u001B[0m     AbstractTensorNetwork, \n\u001B[1;32m      4\u001B[0m     ContractionTree, \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     tensor_contraction_sparse\n\u001B[1;32m     10\u001B[0m )\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deepcopy\n",
      "File \u001B[0;32m~/nfs_share/nfs/ljh/ASCpro/artensor/examples/load_circuits.py:6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deepcopy\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcirq\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mswap_seq\u001B[39m(L, idx0, idx1):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m idx1 \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m L \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cirq'"
     ]
    }
   ],
   "source": [
    "from load_circuits import QuantumCircuit\n",
    "from artensor import (\n",
    "    AbstractTensorNetwork, \n",
    "    ContractionTree, \n",
    "    find_order, \n",
    "    contraction_scheme,\n",
    "    tensor_contraction,\n",
    "    contraction_scheme_sparse,\n",
    "    tensor_contraction_sparse\n",
    ")\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we load the sycamore quantum circuit with $n=30$, $m=14$ and EFGH sequence. `sc_target=30` means the largest size of intermediate tensors is $2^{30}$ (since the data type is `complex64`, it will take about 8G of memory). Thus, in order to perform such a contraction, you need a GPU with memory larger than 24G (`einsum` operator in pytorch need to take 3 times of involving tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m, seq, device, sc_target, seed = 30, 14, 'EFGH', 'cuda', 30, 0\n",
    "qc = QuantumCircuit(n, m, seq=seq)\n",
    "edges = []\n",
    "for i in range(len(qc.neighbors)):\n",
    "    for j in qc.neighbors[i]:\n",
    "        if i < j:\n",
    "            edges.append((i, j))\n",
    "neighbors = list(qc.neighbors)\n",
    "final_qubits = set(range(len(neighbors) - n, len(neighbors)))\n",
    "tensor_bonds = {\n",
    "    i: [edges.index((min(i, j), max(i, j))) for j in neighbors[i]] \n",
    "    for i in range(len(neighbors)) if i not in final_qubits\n",
    "} # open tensor network without final state\n",
    "bond_dims = {i:2.0 for i in range(len(edges))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contract the open tensor network corresponding to the quantum circuit, to get the overall $2^{30}$ amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_slicing, slicing_bonds, ctree_new = find_order(\n",
    "    tensor_bonds, bond_dims, seed, \n",
    "    sc_target=sc_target, trials=5, \n",
    "    iters=10, slicing_repeat=1, betas=np.linspace(3.0, 21.0, 61)\n",
    ")\n",
    "print('order_slicing =', order_slicing)\n",
    "print('slicing_bonds =', slicing_bonds)\n",
    "\n",
    "tensors = []\n",
    "for x in range(len(qc.tensors)):\n",
    "    if x not in final_qubits:\n",
    "        tensors.append(qc.tensors[x].to(device))\n",
    "\n",
    "scheme, bonds_final = contraction_scheme(ctree_new)\n",
    "\n",
    "final_qubits = sorted(final_qubits)\n",
    "permute_dims = [0] * len(final_qubits)\n",
    "for x in range(len(bonds_final)):\n",
    "    _, y = edges[bonds_final[x]]\n",
    "    permute_dims[list(final_qubits).index(y)] = x\n",
    "full_amps = tensor_contraction(\n",
    "    deepcopy(tensors), scheme\n",
    ").permute(permute_dims).reshape(-1).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of the relation between slicing edges and fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_edges_manually_select = [\n",
    "    (44, 66), (46, 66), (69, 94), (81, 94), \n",
    "    (88, 99), (94, 116), (111, 127), (112, 122)\n",
    "]\n",
    "\n",
    "tensors_slicing = deepcopy(tensors)\n",
    "slicing_indices = {}\n",
    "neighbors_copy = deepcopy(neighbors)\n",
    "tensor_network = AbstractTensorNetwork(\n",
    "    deepcopy(tensor_bonds), \n",
    "    deepcopy(bond_dims))\n",
    "\n",
    "while len(slicing_edges_manually_select):\n",
    "    slicing_edge = slicing_edges_manually_select.pop(0)\n",
    "    x, y = slicing_edge\n",
    "    idx_x_y = neighbors_copy[x].index(y)\n",
    "    idx_y_x = neighbors_copy[y].index(x)\n",
    "    neighbors_copy[x].pop(idx_x_y)\n",
    "    neighbors_copy[y].pop(idx_y_x)\n",
    "    slicing_indices[(x, y)] = (idx_x_y, idx_y_x)\n",
    "    tensors_slicing[x] = tensors_slicing[x].select(idx_x_y, 0)\n",
    "    tensors_slicing[y] = tensors_slicing[y].select(idx_y_x, 0)\n",
    "\n",
    "    tensor_network.slicing(edges.index(slicing_edge))\n",
    "    ctree_appro = ContractionTree(deepcopy(tensor_network), order_slicing, 0)\n",
    "    scheme, _ = contraction_scheme(ctree_appro)\n",
    "    appro_amps = tensor_contraction(\n",
    "        deepcopy(tensors_slicing), scheme\n",
    "    ).permute(permute_dims).reshape(-1).cpu()\n",
    "    fidelity = (\n",
    "        (full_amps.conj() @ appro_amps.reshape(-1)).abs() /\n",
    "        (full_amps.abs().square().sum().sqrt() * appro_amps.abs().square().sum().sqrt())\n",
    "    ).square().item()\n",
    "    \n",
    "    print(\n",
    "        'after slicing {} edges, fidelity now is {:.5f} (estimated value {})'.format(\n",
    "            len(slicing_indices), fidelity, 1/2**(len(slicing_indices))\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the amplitudes calculated by Google using Schrodinger-Feynman algorithm as the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples(filename):\n",
    "    import os\n",
    "    if os.path.exists(filename):\n",
    "        samples_data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            l = f.readlines()\n",
    "        f.close()\n",
    "        for line in l:\n",
    "            ll = line.split()\n",
    "            samples_data.append((ll[0], float(ll[1]) + 1j*float(ll[2])))\n",
    "        return samples_data\n",
    "    else:\n",
    "        raise ValueError(\"{} does not exist\".format(filename))\n",
    "\n",
    "data = read_samples('amplitudes_n30_m14_s0_e0_pEFGH_10000.txt')\n",
    "max_bitstrings = 1_000\n",
    "bitstrings = [data[i][0] for i in range(max_bitstrings)]\n",
    "amplitude_google = np.array([data[i][1] for i in range(max_bitstrings)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the result calculated by sparse-state is identical to Goggle's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensors_sparsestate = []\n",
    "for i in range(len(qc.tensors)):\n",
    "    if i in final_qubits:\n",
    "        tensors_sparsestate.append(\n",
    "            torch.tensor([[1, 0], [0, 1]], dtype=torch.complex64, device=device)\n",
    "        )\n",
    "    else:\n",
    "        tensors_sparsestate.append(qc.tensors[i].to(device))\n",
    "\n",
    "tensor_bonds = {\n",
    "    i:[edges.index((min(i, j), max(i, j))) for j in neighbors[i]] \n",
    "    for i in range(len(neighbors))\n",
    "} # now all tensors will be included\n",
    "\n",
    "order_slicing, slicing_bonds, ctree_new = find_order(\n",
    "    tensor_bonds, bond_dims, final_qubits, seed, max_bitstrings, \n",
    "    sc_target=sc_target, trials=5, iters=10, slicing_repeat=1, \n",
    "    betas=np.linspace(3.0, 21.0, 61)\n",
    ")\n",
    "\n",
    "scheme_sparsestate, _, bitstrings_sorted = contraction_scheme_sparse(\n",
    "    ctree_new, bitstrings, sc_target=sc_target\n",
    ")\n",
    "\n",
    "slicing_edges = [edges[i] for i in slicing_bonds]\n",
    "slicing_indices = {}.fromkeys(slicing_edges)\n",
    "neighbors_copy = deepcopy(neighbors)\n",
    "for i, j in slicing_edges:\n",
    "    idxi_j = neighbors_copy[i].index(j)\n",
    "    idxj_i = neighbors_copy[j].index(i)\n",
    "    neighbors_copy[i].pop(idxi_j)\n",
    "    neighbors_copy[j].pop(idxj_i)\n",
    "    slicing_indices[(i, j)] = (idxi_j, idxj_i)\n",
    "\n",
    "\n",
    "amplitude_sparsestate = torch.zeros(\n",
    "    (len(bitstrings),), dtype=torch.complex64, device=device\n",
    ")\n",
    "for s in range(2**len(slicing_edges)):\n",
    "    configs = list(map(int, np.binary_repr(s, len(slicing_edges))))\n",
    "    sliced_tensors = tensors_sparsestate.copy()\n",
    "    for i in range(len(slicing_edges)):\n",
    "        m, n = slicing_edges[i]\n",
    "        idxm_n, idxn_m = slicing_indices[(m, n)]\n",
    "        sliced_tensors[m] = sliced_tensors[m].select(idxm_n, configs[i]).clone()\n",
    "        sliced_tensors[n] = sliced_tensors[n].select(idxn_m, configs[i]).clone()\n",
    "    amplitude_sparsestate += tensor_contraction_sparse(\n",
    "        sliced_tensors, scheme_sparsestate\n",
    "    )\n",
    "\n",
    "correct_num = 0\n",
    "for i in range(len(bitstrings_sorted)):\n",
    "    ind_google = bitstrings.index(bitstrings_sorted[i])\n",
    "    relative_error = abs(\n",
    "        amplitude_sparsestate[i].item() - amplitude_google[ind_google]\n",
    "    ) / abs(amplitude_google[ind_google])\n",
    "    if relative_error <= 0.05:\n",
    "        correct_num += 1\n",
    "print(f'bitstring amplitude correct ratio:{correct_num/max_bitstrings}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "f95100793b43010e28ed80d490b665ccbe9af4299329cffb7252da6f38962fd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
